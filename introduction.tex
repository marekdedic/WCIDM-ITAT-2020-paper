\section{Introduction}

Multi-instance learning (MIL) belongs to recently fast developing areas of machine learning. Though it is a supervised learning method, this paper addresses its application to unsupervised learning -- clustering. Whereas traditional clustering is one of points, multi-instance clustering clusters multisets of points, also known as bags. Such a grouping of the points into bags is considered a property of the problem at hand and therefore sourced from the input data.

While there have been some previous attempts at multi-instance clustering, they use pairwise relations between all points in all bags, quickly becoming unwieldy and computationally infeasible. Our work uses multi-instance learning as a general toolkit for learning representations of bags and then clusters the bags using those representations. This builds on previous works by the authors and enables clustering of arbitrary data-structures by expressing them as hierarchies of multi-instance problems and then using the internal structure to better solve the problems at hand.

Multi-instance clustering is evaluated in Section~\ref{sec:experimental-comparison} in the application domain of computer and network security. While this is only one of many possible applications of MIL due to its general expressive power for structured data, it is the domain of choice for the authors. Here, MIL is used to represent user activity as a bag of network connections for each user in a fixed time window. For clustering, this enables detection of compromised users based on their complex behaviours. Clustering opens a new window of opportunity here by e.g. grouping servers with similar behaviour together, allowing a human analyst to boost their work significantly by deciding about whole clusters of servers instead of each one individually.

A key prerequisite for the application of multi-instance learning to clustering is that loss functions for clustering, originally proposed for points, are adapted for bags. In this paper, such an adaptation is outlined for three sophisticated loss functions: contrastive predictive coding, triplet loss and magnet loss.

Of the three proposed methods, triplet loss and magnet loss utilize some labels as part of the training process and so are not truly unsupervised. As such, the authors expect them to outperform the method based on contrastive predictive coding, which on the other hand is the most promising and innovative approach from the theoretical point of view.

In the next section, basic properties of multi-instance learning and clustering are briefly introduced. The adaptation of the three considered loss functions is explained in Section~\ref{sec:loss-functions}. A comprehensive comparison of them is then presented in Section~\ref{sec:experimental-comparison}.
