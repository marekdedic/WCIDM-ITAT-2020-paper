\section{Introduction}

Multi-instance learning (MIL) belongs to recently fast developing areas of machine learning. Though it is a supervised learning method, this paper addresses its application to unsupervised learning -- clustering. Whereas traditional clustering clusters points, multi-instance clustering clusters multisets of points, also known as bags. Such a grouping of the points into bags is considered a property of the problem at hand and therefore sourced from the input data.

Multi-instance learning is general enough that an exploration of its utility to other tasks is natural. In this paper, the formalism is applied to the task of clustering. While there have been some previous attempts at this, they use pairwise relations between all points in all bags, quickly becoming unwieldy and computationally infeasible. Our work uses multi-instance learning as a general toolkit for learning representations of bags and then clusters the bags using those representations. This builds on previous works by the authors and enables clustering of arbitrary data-structures by expressing them as hierarchies of multi-instance problems and then using the internal structure to better solve the problems at hand.

A key prerequisite for the application of multi-instance learning to clustering is that loss functions for clustering, originally proposed for points, have to be adapted for bags. In this paper, such an adaptation is outlined for three sophisticated loss functions: contrastive predictive coding, triplet loss and magnet loss. From the point of view of their adaptation, two properties of bags are crucial:
\begin{romanitems}
	\item Bags can be incorporated into the statistical framework, viewing data as realizations of random variables. In this framework, bags are simply random samples.
	\item Bags are mapped back to the instance space using various aggregation functions, for example, in the case of numerical instances, such functions as mean, min and max. Aggregation functions allow transfering important mappings defined on the instance space to bags. Among such mappings are embeddings resulting from representation learning by neural networks.
\end{romanitems}

In the next section, basic properties of multi-instance learning and clustering are briefly introduced. The adaptation of the considered loss functions is explained in Section~\ref{sec:loss-functions}. A comprehensive comparison of them is then presented in Section~\ref{sec:experimental-comparison}.
