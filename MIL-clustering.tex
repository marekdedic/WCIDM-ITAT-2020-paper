\name{Multi-instance learning} (MIL) was first described by \cite{dietterich_solving_1997}. In its original form, it was developed and used for \name{supervised learning}. Some prior art also exists for \name{unsupervised learning}, such as \cite{chen_contextual_2012}.

The MIL paradigm is a type of representation learning on data with some internal structure. Therefore, it views a sample as a \name{bag} (i.e. a multiset) of an arbitrary number of objects. The basic elements of MIL are samples from a space \( \mathspace{X} \) and their corresponding labels from a space \( \mathspace{Y} \) (a space of classes). Compared to usual supervised learning, MIL replaces individual instances with bags of instances from the space \( \mathspace{X} \) such that every instance in \( \mathspace{X} \) belongs to at least one bag from the bag-space \( \mathspace{B} \).

\cite{dietterich_solving_1997} provides an example of a multi-instance problem where each bag represents a key chain with some keys (instances). To solve the problem of finding which key opens a particular lock, a \enquote{proxy} MIL problem is presented -- determining which key chain opens that particular lock. This line of thinking leads to the pivotal definition of a label of the bag as a maximum over the labels of instances in the bag. In later works such as \cite{pevny_using_2017}, this interpretation of MIL is abandoned in favor of a more general one. An instance is no longer viewed as having a meaning in and of itself, but only in the context of its bag. The notion of an instance-level label is dropped as in this interpretation, the bag is the atomic unit of interest. To this end, the embedded-space paradigm, described in section \ref{sec:embedded-space-paradigm}, is used.

\subsection{A probabilistic formulation of multi-instance learning}\label{sec:probabilistic-formalism}
A probabilistic way of describing multi-instance learning was first introduced in \cite{pevny_using_2017} and builds on the previous work \cite{muandet_learning_2012}.

Let for the space \( \mathspace{X} \) exist a measurable space \( \left( \mathspace{X}, \mathspace{A} \right) \), where \( \mathspace{A} \) is a \( \sigma \)-algebra on \( \mathspace{X} \). Let \( \mathcal{P}^\mathspace{X} \) denote the set of all probability measures on \( \left( \mathspace{X}, \mathspace{A} \right) \). A bag \( B \) is viewed as a set of realizations of a particular probability distribution \( P \in \mathcal{P}^\mathspace{X} \), that is
\[ B = \left\{ x_i \middle| x_i \sim P, i \in \left\{ 1, \dots, m \right\}, m \in \mathfield{N} \right\} \]
A core assumption of this formalism is that \( P = P \left( P_B, y_B \right) \) where \( P_B \) is the probability distribution of instances in the bag \( B \) and \( y_B \) is the label assigned to \( B \). That is, the probability distribution of the bag is dependent on its assigned label.

\subsection{Embedded-space paradigm for solving multi-instance problems}\label{sec:embedded-space-paradigm}
While it is possible to use several approaches to solving multi-instance problems, in this work, the embedded-space paradigm was used. For an overview of the other paradigms, see \cite{dedic_optimalization_2020}.

In the embedded space paradigm, labels are only defined on the level of bags. In order for these bag labels to be learned, an embedding function of the form \( \phi : \mathspace{B} \to \bar{\mathspace{X}} \) must be defined, where \( \bar{\mathspace{X}} \) is a latent space, which may or may not be identical to \( \mathspace{X} \). Using this function, each bag can be represented by an object \( \phi \left( B \right) \in \bar{\mathspace{X}} \), which makes it possible to use any off-the-shelf supervised learning algorithm acting on \( \bar{\mathspace{X}} \). Among the simplest embedding functions are e.g. element-wise minimum, maximum and mean. A more complicated embedding function may for example apply a neural network to each instance of the bag and subsequently pool the instances using one of the aforementioned functions. More precisely, \cite{pevny_nested_2020} present an approach to learning to classify HTTP traffic by utilizing sets of URLs. Neural networks are used to transform both instance-level and bag-level representations. The model is as follows. A neural network \( f_I : \mathspace{X}_1 \to \mathspace{X}_2 \) is used to transform instances, followed by an aggregation function
\[ g: \mathcal{P}^M \left( \mathspace{X}_2 \right) \to \bar{\mathspace{X}}_1 \ \]
Finally, a second deep neural network \( f_B : \bar{\mathspace{X}}_1 \to \bar{\mathspace{X}}_2 \) is used to transform the representation of each bag. Combining these functions gives the embedding function
\[ \phi \left( B \right) = f_B \left( g \left( \left\{ f_I \left( \mathvec{x} \right) \middle| \mathvec{x} \in B \right\} \right) \right) \]
The functions \( f_I \) and \( f_B \) are realized by a deep neural network using ReLU as its activation function. The aggregation \( g \) is realized as an element-wise mean or maximum of all the vectors.

The structure of the data is highly exploited using the embedded-space paradigm. The approach uses multiple layers of MIL nested in each other -- the instances of a bag do not necessarily need to be feature vectors, but can be bag in themselves. The HTTP traffic of a particular client is therefore represented as a bag of all second-level domains the client has exchanged datagrams with. Each second-level domain is then represented as a bag of individual URLs which the client has connected to. Individual URLs are then split into 3 parts, domain, path and query, and each part is represented as a bag of tokens, which can then be broken down even further. In the end, the model consists of 5 nested MIL problems.

An added benefit of MIL is also its comparatively easy explainability and interpretability. The authors of \cite{pevny_nested_2020} present a way to extract indicators of compromise and explain the decision using the learned MIL model.

\subsection{Clustering}
Clustering is a prime example of a problem typically associated with unsupervised learning. \cite{xu_comprehensive_2015} even call clustering the most important question of unsupervised learning. The problem at hand, however, is not one of clustering ordinary number vectors in a linear space. Instead, a clustering of objects represented by bags is explored, as that is the problem solved for datasets introduced later in section \ref{sec:experimental-comparison}. While \cite{wang_solving_2000} present a clustering of bags using a modified Hausdorff distance on bags and \cite{kohout_network_2018} present a clustering using maximum mean discrepancy, a different approach to clustering of bags was used in this work. Among the main reasons for this choice is the prohibitively high computational complexity of the previously mentioned approaches on large datasets and the possibility of utilizing the representations previously introduced in \cite{pevny_nested_2020}.

An approach based on the embedded-space paradigm for MIL was chosen. In order to utilize the structure of the data, a MIL model is used to represent each bag in the latent space \( \bar{\mathspace{X}} \). This presents the issue of how to train the embedding function \( \phi \), because its learning in standard MIL is supervised.

The embedding function \( \phi \) can be actually written as \( \phi = \phi \left( B, \mathvec{\theta} \right) \) where \( B \in \mathspace{B} \) and \( \mathvec{\theta} \) are the parameters of the embedding, typically learned during the training phase. In the context of clustering, these parameters still need to be learned over some kind of training. This itself presents another challenge though -- off-the-shelf algorithms typically work in some constant Hilbert space, whereas the latent space needs to change over the learning period. As is the case for MIL itself, end-to-end learning is used. A clustering-loss function \( L_C \) is chosen such that
\[ L_C: \mathcal{P}^M \left( \bar{\mathspace{X}} \right) \to \mathfield{R} \]
Given such clustering-loss function, an actual loss for the embedding model \( \phi \) and its parameters \( \mathvec{\theta} \) can be computed as
\[ L \left( \phi, \mathvec{\theta} \right) = L_C \left( \left\{ \phi \left( B, \mathvec{\theta} \right) \middle| B \in \mathspace{B} \right\} \right) \]
If the cluster-loss \( L_C \) is chosen correctly, minimizing \( L \) over the learning period will yield a latent space \( \bar{\mathspace{X}} \) in which the bags are already naturally clustered according to the design of the cluster-loss function. Applying any off-the-shelf clustering algorithm on \( \bar{\mathspace{X}} \) will then give good results. How to choose the cluster-loss function \( L_C \) is the focus of section \ref{sec:loss-functions}.

\subsection{Clustering evaluation metrics}\label{sec:clustering-metrics}
In our research, several metrics are used. The primary metrics are based on the Silhouette coefficient, the secondary ones on the kNN algorithm. The metrics are somewhat different to the ones traditionally used in evaluating clustering methods as all the datasets used in this work are originally classification datasets and therefore have classes available. Each class can then be viewed as a cluster target and the learned clustering evaluated against these targets.

The first two metrics measure the \name{homogeneity} of clusters (that is the property of instances of one cluster being close to one another) and their \name{separation} (that is the property of instances of different clusters being far apart), see \cite{everitt_cluster_2001}. To measure the homogeneity of a cluster, the average distance between items in a cluster is measured and averaged over all clusters, giving the following metric for items \( x_j \) in clusters \( C_i \):
\[ \mu \left( C_i \right) = \frac{1}{\left\lvert C_i \right\rvert} \sum_{x_j \in C_i} x_j \]
\[ \mathrm{homo} \left( C_1, \dots, C_n \right) = \frac{1}{n} \sum_{i = 1}^n \sum_{x_j \in C_i} \left\lVert x_j - \mu \left( C_i \right) \right\rVert \]

To measure the separation of clusters, the average distance between cluster centres was taken:
\[ \mathrm{sep} \left( C_1, \dots, C_n \right) = \frac{2}{n \left( n - 1 \right)} \sum_{\substack{i, j \in \hat{n} \\ i < j }} \left\lVert \mu \left( C_i \right) - \mu \left( C_j \right) \right\rVert \]

The final primary metric is the Silhouette coefficient:
\[ \mathrm{silhouette} \left( C_1, \dots, C_n \right) = \frac{\mathrm{sep} \left( C_1, \dots, C_n \right)}{\mathrm{homo} \left( C_1, \dots, C_n \right)} \]

For the secondary metrics, the kNN algorithm in the representation space is used and its accuracy measured. The training data of the kNN classifier is in this work referred to as \name{seed data} to avoid confusion with the data used to train the embedding. This gives an insight into the robustness of the clustering, as high-quality embeddings would need only a small amount of seed data points to reach relatively high accuracy. For this measure, the kNN algorithm was run thrice and the results averaged to compensate for high dependence on the particular seed points chosen.
